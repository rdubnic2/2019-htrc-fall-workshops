{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to geocoding and plotting using HTRC Analytics, Geopy and Cartopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the goal is to introduce you to geolocation and plotting locations generated from Named Entity Recognition (NER) using Python, HathiTrust data and HathiTrust Research Center (HTRC) tools. The main focus will be to take the output of [HTRC's built-in Named Entity Recognizer ](https://analytics.hathitrust.org/statisticalalgorithms) algorithm run over a workset, and use it to geocode and plot locations. The NER algorithm's output is very useful for this, as it does the NER step and tagging for you, without any need for code. However, should you be interested, at the end of the notebook, there is a further section on recreating the NER steps over full text data directly using the [Natural Language Toolkit (NLTK)](https://www.nltk.org/). This notebook will help you:\n",
    "\n",
    "1. Load the output CSV of the NER algorithm in HTRC Analytics into a [Pandas](https://pandas.pydata.org/) DataFrame and extract locations;\n",
    "2. Use the library [GeoPy](https://geopy.readthedocs.io/en/stable/) to geocode each location; and\n",
    "3. Use a mapping library, [Cartopy](https://scitools.org.uk/cartopy/docs/latest/), to plot geographic data using MatPlotLib.\n",
    "\n",
    "If you're using Anaconda to manage your python packages, the Natural Language Toolkit (NLTK), Cartopy, Matplotlib and Pandas should already be installed. You can test this by skipping to our first code cell and seeing if the libraries import correctly. Instructions for installing these packages and Geopy using `pip` are below.\n",
    "\n",
    "NLTK, GeoPy, Matplotlib and Pandas can be downloaded and installed using `pip`, as so:\n",
    "\n",
    "`pip install geopy`\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "`pip install matplotlib`\n",
    "\n",
    "`pip install nltk`\n",
    "\n",
    "`pip install cartopy`\n",
    "\n",
    "If you're using Anaconda but do not have certain packages installed, it's recommended to download and install using `conda` instead of `pip` as it will ensure that the correct version and dependencies are also installed:\n",
    "\n",
    "`conda install -c conda-forge cartopy`\n",
    "\n",
    "`conda install -c conda-forge matplotlib`\n",
    "\n",
    "`conda install -c conda-forge geopy`\n",
    "\n",
    "`conda install nltk`\n",
    "\n",
    "`conda install pandas`\n",
    "\n",
    "Now that we are all setup, we can start on this project. First, there are some libraries we must import into this notebook in order to use them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy, time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NER data from HTRC Analytics\n",
    "\n",
    "Now, let's load in the result of our NER algorithm run in HTRC Analytics: a CSV of entities, the volume and page of their appearance, and their entity type. We'll import this as a Pandas DataFrame, as it is already in CSV form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle # importing another library so we can randomly shuffle our entities DF \n",
    "                                  # to get results from all volumes in a sub-set\n",
    "\n",
    "entities = open('entities-modern-traveller.csv','r')\n",
    "ent_df = pd.read_csv(entities)\n",
    "\n",
    "ent_df = shuffle(ent_df)\n",
    "\n",
    "print(ent_df.shape)\n",
    "ent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the CSV was loaded correctly and get to see what our data looks like. Our entities are tagged with strings in the `type` column, so we need to go through our data and pull out the entities that are tagged as \"LOCATION\" in this colummn. If we want to be inclusive, with small adjustments to our code we could also pull out other types, such as \"ORGANIZATION\" in the interest of not losing any positives. However, we'll focus just on locations today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and geocoding locations from our entities CSV\n",
    "   To extract only the entities tagged as locations in this file, we can simply use a `for` loop and the Pandas method `.iterrows()` which will let us use the index of the DataFrame to loop through the data, but still individually call any columns in which we're interested. Here, we want to extract the entity name for each entity with the value `LOCATION` in the `type` column. Eventually, we'll end with a new list of only place names (`place_list`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_list = []\n",
    "\n",
    "for i, r in ent_df.iterrows():\n",
    "    name = r['entity']\n",
    "    ner_type = r['type']\n",
    "    if ner_type == 'LOCATION':\n",
    "        place_list.append(name)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(f\"Found {len(place_list)} locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geopy allows for a number of different types of geocoders to be initialized and called, such as Google Maps, Bing Maps, Nominatim, etc. Each may have different requirements for their use, however, such as Google Maps, which requires an API key. In order to avoid this, we'll use Nominatim, which doesn't require a key and is built on the [OpenStreetMap](https://www.openstreetmap.org) data, which is crowdsourced, open and free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "api = geopy.geocoders.Nominatim(user_agent=\"my-application\") # initializing Nominatim as our geocoding engine.\n",
    "    \n",
    "result, (lat, lng) = api.geocode(\"1600 Pennsylvania Ave NW, Washington D.C.\")\n",
    "print(result)\n",
    "print(lat, lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've initialized our geocoder object using Nominatim, and for easier referencing later, named it `api`. We can now use all of the built-in methods that geopy has for geocoders by only calling api + the method syntax.\n",
    "\n",
    "Just to ensure things are working, I've also included a test geocode: the address for the White House, which it correctly finds. You'll notice three variables are being retrieved from the geocode: \n",
    "\n",
    "* `result`: the place name of the geolocation\n",
    "* `lat`: the latitude for the geolocation\n",
    "* `lng`: the longitude for the geolocation\n",
    "\n",
    "This syntax is handy, as it also assigns the output from the geocoder to variables (`result`, `lat`, `lng`) instantly. Additionally, we could also feed the geocoder a place name and see if it can generate an address and coordinates instead of the inverse. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, (lat,lng) = api.geocode('White House')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for place names with some specificity, this is a workable method of geocoding (and in fact is what we'll be doing to geocode the locations we extract from our CSV!). However, you should remember that as the place name becomes more ambiguous, the geocoder may not necessarily throw an error, but instead retrieve the wrong location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, (lat,lng) = api.geocode('Assembly Hall') # U. Illinois' (and, annoyingly, Indiana U.'s!) basketball stadium\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot our locations, we must also include an intermediate step to extract and compile the coordinates of the locations in our CSV into a new list, `place_coord`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "place_coord = []\n",
    "\n",
    "for placename in place_list[:75]:\n",
    "    # time.sleep(1)\n",
    "    try:\n",
    "        fullname, (lat, lng) = api.geocode(placename)\n",
    "        # print(placename + \":  \" + fullname)\n",
    "        # print(lat, lng)\n",
    "        place_coord.append((lng, lat))\n",
    "    except:\n",
    "        print('Could not find: ' + placename)\n",
    "        \n",
    "print(f\"Found {len(place_coord)} locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has an optional rest command in the loop in case you want to be a friendly and cautious API user and not worry about overloading the servers of the API provider. I've also added some print statements in this loop to see which locations aren't found, which is recommended. Since programmatic NER is dealing with textual vagueries just like humans do, there are bound to be some locations that are not found when geocoding, or entities that are incorrectly tagged as locations, or just incorrectly tagged altogether. This allows for a spot check on anything that might be going wrong, as well as a spot-check to see if the missed locations are false negatives or were false positives when identified as locations. Both of these look like they could be correctly tagged as locations, but also seem vague enough that unsuccessfully geocoding them seems possible. Each researcher will have a different opinion about ensuring if all outliers are identified, but for this exercise it's okay for these locations to be omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting our locations using Cartopy\n",
    "Now that we have place names and coordinates, we need only plot them. We'll use Cartopy, a specialized library for plotting geographic data using matplotlib. In addition to using much of the same syntax as matplotlib, Cartopy has some nice built-in features that we'll show below. Of course, any plotting library or method could be substituted once you have a list of coordinates. For Cartopy in Jupyter, it works best for inline graphics if you initialize the figure first, with set dimensions, and then generate the data to be plotted on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree()) # choosing a Plate Carree map projection as our base\n",
    "# ax.stock_img()\n",
    "ax.coastlines(resolution='10m') # picking the highest resolution coastlines to show\n",
    "# ax.set_global() # this shows the entire global map on our plot as compared to a scaled subset that covers all points\n",
    "\n",
    "for lon, lat in place_coord:\n",
    "    plt.scatter(lon, lat, s=15, color='red', transform=ccrs.PlateCarree()) # 's' is the parameter that determines size of points on plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Our map (a [Plate Carree](https://en.wikipedia.org/wiki/Equirectangular_projection) projection) now has each of the 48 locations found in our 47-volume adventure novel workset plotted. We can experiment with different views by choosing a different projection, changing the color/resolution of certain parameters, or by disabling others. For instance, what do you think happens if you comment out (to do so, use `command` + `/` while the text is highlighted), the `ax.set_global()` line? Were your expectations right? What do you think will happen if we do the same with `ax.stock_img()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, Cartopy has a lot of built-in options to produce different types of map graphics in python. Below are some of the ones I found interesting, but more can be found in the Cartopy documentation linked at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "# ax.stock_img()\n",
    "ax.coastlines(resolution='10m')\n",
    "# ax.set_global()\n",
    "\n",
    "for lon, lat in place_coord:\n",
    "    plt.scatter(lon, lat, color='red', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Cartopy allows you to download non-standard datasets from Natural Earth (http://www.naturalearthdata.com/) \n",
    "# to include in our plot, such as the highest resolution representation of rivers:\n",
    "\n",
    "# rivers_10m = cfeature.NaturalEarthFeature('physical', 'rivers_lake_centerlines', '10m')\n",
    "\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black', facecolor=cfeature.COLORS['land'])\n",
    "ax.add_feature(cfeature.LAKES, edgecolor='black')\n",
    "# ax.add_feature(rivers_10m, facecolor='none', edgecolor='b')\n",
    "ax.add_feature(cfeature.BORDERS)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.STATES)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating this process for full text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce this process and output without a list or CSV of entities already in hand, we can use the [Natural Language Toolkit (NLTK)](https://www.nltk.org/), which runs best over full text documents. However, we must first also pre-process our text to give the best chance for accurate NER. In this instance, we're going to tokenize and tag each token with a part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sample_string = 'I live in Champaign, but I often travel to Chicago, Istanbul, and London.'\n",
    "words = nltk.word_tokenize(sample_string)\n",
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that we have a list with our tokens and their part-of-speech (POS) tags (which are based on the [Penn Treebank POS tags](https://www.clips.ua.ac.be/pages/mbsp-tags)), which can be fed directly into NLTK's built-in named entity classifier, `nltk.ne_chunk()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sample_chunked = nltk.ne_chunk(tagged)\n",
    "# sample_chunked.draw()\n",
    "print(sample_chunked)\n",
    "print(type(sample_chunked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the actual named entities that the classifier is tagging, as well as the type of the output object: `<class 'nltk.tree.Tree'>` which you can probably tell is an NLTK Tree. If you'd like to see the drawn tree, you can un-comment the print statement in the above code, and the tree will be drawn in a new, pop-up window.\n",
    "\n",
    "This type of object is specific to NLTK, so we'll need to think about how we can take the tree of entities and end up with a list of the desired class of entities we'd like, here roughly defined as place names. In order to extrsact locations, a less granular entity tag than NLTK uses, we'll need to write a function to identify the items in an NLTK tree with the type 'GPE' (geo-political entity) or 'ORGANIZATION' (or other tags, as appropriate). This function takes an NLTK Tree object as input, which minimzes steps in finding the entities we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code here is adapted from Ted Underwood, who himself worked off a Stack Overflow answer by alvas:\n",
    "# http://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "\n",
    "def get_placenames(parsed_tree):\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for node in parsed_tree:\n",
    "        if type(node) == nltk.Tree and (node.label() == \"GPE\" or node.label() == 'ORGANIZATION'):\n",
    "            current_chunk.append(\" \".join([token for token, pos in node.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "\n",
    "print(get_placenames(sample_chunked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd encourage you to go through the function and see if you can identify what is happening at each step, but roughly, this function takes a parsed NLTK tree as an input, and then navigates through each node on the tree looking for entities with labels of a given type that we are defining (line 9), here `GPE` or `ORGANIZATION` which can both be used for locations (though remember that *all* tags are <100% reliable, since they are generated using trained classifiers).\n",
    "\n",
    "Now that we habve a function that will find and extract particular types of entities from an NLTK tree, we can test it out on a block of text, here an excerpt from Herman Melville's *Moby Dick*, which nicely lends itself to NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ' '.join([\"I stuffed a shirt or two into my old carpet-bag, tucked it \",\n",
    "        \"under my arm, and started for Cape Horn and the Pacific. Quitting \",\n",
    "        \"the good city of old Manhatto, I duly arrived in New Bedford. \",\n",
    "        \"It was a Saturday night in December. Much was I disappointed \",\n",
    "        \"upon learning that the little packet for Nantucket had already sailed, \",\n",
    "        \"and that no way of reaching that place would offer, till the following \",\n",
    "        \"Monday. As most young candidates for the pains and penalties of whaling \",\n",
    "        \"stop at this same New Bedford, thence to embark on their voyage, it may \",\n",
    "        \"as well be related that I, for one, had no idea of so doing. For \",\n",
    "        \"my mind was made up to sail in no other than a Nantucket craft, \",\n",
    "        \"because there was a fine, boisterous something about everything \",\n",
    "        \"connected with that famous old island, which amazingly pleased me. \",\n",
    "        \"Besides though New Bedford has of late been gradually monopolising \",\n",
    "        \"the business of whaling, and though in this matter poor old Nantucket \",\n",
    "        \"is now much behind her, yet Nantucket was her great original - the Tyre \",\n",
    "        \"of this Carthage;â€”the place where the first dead American whale was \",\n",
    "        \"stranded. Where else but from Nantucket did those aboriginal whalemen, \",\n",
    "        \"the Red-Men, first sally out in canoes to give chase to the Leviathan? \",\n",
    "        \"And where but from Nantucket, too, did that first adventurous little sloop \",\n",
    "        \"put forth, partly laden with imported cobblestones - so goes the story - \",\n",
    "        \"to throw at the whales, in order to discover when they were nigh enough \",\n",
    "        \"to risk a harpoon from the bowsprit?\"])\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Though it's a bit counterintuitive, given that we join the lines above into one chunk of text, before we \n",
    "# proceed to word_tokenize, we break the block of text into a list of sentences. The tagging and chunking \n",
    "# processes tend to work better on strings that are single sentences.\n",
    "\n",
    "# Then we loop through the sentences, tagging and chunking each one, extracting placenames, and adding those \n",
    "# to a growing list of moby_places. Note that \"extend\" is like \"append\" but appends all the elements of a list \n",
    "# rather than a single one, which is useful for this task since we have potentially multiple entities we're\n",
    "# interested in for each chunk.\n",
    "\n",
    "moby_places = []\n",
    "\n",
    "for s in sentences:\n",
    "    words = nltk.word_tokenize(s)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    namedEnt = nltk.ne_chunk(tagged)\n",
    "    chunks = get_placenames(namedEnt)\n",
    "    moby_places.extend(chunks)\n",
    "\n",
    "print('\\nPLACENAMES: ')\n",
    "print(moby_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Well, sort of. See anything strange with the above place names? This is a good time to add another friendly reminder that classifiers *will* make mistakes!\n",
    "\n",
    "With a list of place names, we can now pick up a similar workflow to the one demonstrated with the HTRC NER output file, and geocode our locations by name using Geopy. This step should look familiar!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allcoordinates = []\n",
    "\n",
    "for placename in moby_places:\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        fullname, (lat, lng) = api.geocode(placename)\n",
    "        print(placename + \":  \" + fullname)\n",
    "        print(lat, lng)\n",
    "        allcoordinates.append((lng, lat))\n",
    "    except:\n",
    "        print('Could not find: ' + placename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see Geopy's best guess as to what each place name is referring to. Any strange or unexpected results?\n",
    "\n",
    "We can also pilot this process for a sample sentence, here the same sentence we used in our first cell in this section to generate POS tags to feed into NLTK. Feel free to modify the sentence and see what the results look like. Are all of the locations found? Any unexpcted results? If so, any ideas why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also try this for our sample sentence:\n",
    "\n",
    "sample_sentences = ['I live in Champaign, but I often travel to Chicago, Istanbul, and London.']\n",
    "sample_places = []\n",
    "\n",
    "for s in sample_sentences:\n",
    "    words = nltk.word_tokenize(s)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    # print(tagged)\n",
    "    namedEnt = nltk.ne_chunk(tagged)\n",
    "    # print(namedEnt)\n",
    "    chunks = get_placenames(namedEnt)\n",
    "    sample_places.extend(chunks)\n",
    "    \n",
    "print(sample_places)\n",
    "\n",
    "sample_coord = []\n",
    "\n",
    "for placename in sample_places:\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        fullname, (lat, lng) = api.geocode(placename)\n",
    "        print(placename + \":  \" + fullname)\n",
    "        print(lat, lng)\n",
    "        sample_coord.append((lng, lat))\n",
    "    except:\n",
    "        print('Could not find: ' + placename)\n",
    "        \n",
    "print(sample_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, with a list of place names and coordinates, we can call on our old electronic friend Cartopy to generate a map and plot the locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "# ax.stock_img()\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.set_global()\n",
    "\n",
    "for lon, lat in sample_coord:\n",
    "    plt.scatter(lon, lat, s=35, color='red', transform=ccrs.PlateCarree())\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success, and this time without the \"sort of!\"\n",
    "\n",
    "If you want to try again or want to explore some of the options with Cartopy (check out the [Cartopy documentation](https://scitools.org.uk/cartopy/docs/latest/index.html) for more information on the full set of features and functionality), try out geocoding and plotting other locations. Here is another example using a list of English Premier League team stadia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try substituting other locations in this list and see what results you get!\n",
    "\n",
    "pl_stadia = ['Anfield', 'Goodison Park', 'Old Trafford', 'City of Manchester Stadium', 'Carrow Road', \n",
    "             'Bramall Lane','Emirates Stadium','Stamford Bridge', 'London Stadium', \"St. James' Park\",\n",
    "             'Dean Court', 'Falmer Stadium', 'King Power Stadium', 'Molineux Stadium', \"Southampton FC\",\n",
    "             'Selhurst Park', 'Tottenham Hotspur Stadium', 'Turf Moor', 'Vicarage Road', 'Villa Park']\n",
    "\n",
    "pl_coord = []\n",
    "\n",
    "for placename in pl_stadia:\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        fullname, (lat, lng) = api.geocode(placename)\n",
    "        print(placename + \":  \" + fullname)\n",
    "        print(lat, lng)\n",
    "        pl_coord.append((lng, lat))\n",
    "    except:\n",
    "        print('Could not find: ' + placename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know a bit more about our data this time, we can try out a few of Cartopy's advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "# ax.stock_img()\n",
    "# ax.set_global()\n",
    "\n",
    "for lon, lat in pl_coord:\n",
    "    plt.scatter(lon, lat, color='red', alpha=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "    \n",
    "rivers_10m = cfeature.NaturalEarthFeature('physical', 'rivers_lake_centerlines', '10m')\n",
    "ocean_10m = cfeature.NaturalEarthFeature('physical', 'ocean', '10m')\n",
    "land_10m = cfeature.NaturalEarthFeature('physical', 'land', '10m')\n",
    "\n",
    "ax.add_feature(ocean_10m, facecolor='blue')\n",
    "ax.add_feature(land_10m, edgecolor = 'black', facecolor='none')\n",
    "ax.add_feature(rivers_10m, facecolor='none', edgecolor='blue')\n",
    "ax.add_feature(cfeature.BORDERS)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on available features is available at the previously linked Cartopy documentation. Cartopy has some built-in support for a few external feature sources as well, including [Natural Earth](https://www.naturalearthdata.com/) features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
